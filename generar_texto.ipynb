{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "generar_texto.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rubenguerra/hello_world/blob/master/generar_texto.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJMk1pYrjzm-",
        "colab_type": "text"
      },
      "source": [
        "PROGRAMA DE RED NEURONAL PARA GENERAR TEXTO TOMANDO COMO BASE LA OBRA DE W. SHAKESPEARE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1VFDYSd3QTXA",
        "colab_type": "code",
        "outputId": "1dbd8204-ac99-4cda-ce70-0bf4cbe5ce59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        }
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "!pip install -q tensorflow-gpu==2.0.0-beta1\n",
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 348.9MB 52kB/s \n",
            "\u001b[K     |████████████████████████████████| 3.1MB 66.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 501kB 50.1MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XzIW5egTRY9d",
        "colab_type": "code",
        "outputId": "43959591-61e6-4be4-e276-c154b3f6f179",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1122304/1115394 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZCE0pKPMSbkc",
        "colab_type": "code",
        "outputId": "13bd5bdd-ecb6-4f92-91f3-5eaed7bc6b14",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "##Leer el texto\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "\n",
        "##La longitud del texto es el número de caracteres en él\n",
        "print('Length of text: {} characters'.format(len(text)))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of text: 1115394 characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UwM6PDQRTc89",
        "colab_type": "code",
        "outputId": "d7721392-ec0d-4c98-b40d-be852b2beb60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "#Toma una muestra de los primeros 259 caracteres en el texto\n",
        "print(text[:250])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2CCGY-_TyWd",
        "colab_type": "code",
        "outputId": "410bbe36-dca7-40a7-8f51-d4df18d9a290",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "##Caracteres únicos en el archivo\n",
        "vocab = sorted(set(text))\n",
        "print('{} caracteres únicos'.format(len(vocab)))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "65 caracteres únicos\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwDNMwbmU95m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##Crea un mapeo de caracteres únicos a indices\n",
        "## Transforma cada caracter en un índice\n",
        "## Luego ese índice es guardado en un arreglo\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "text_as_int = np.array([char2idx[c] for c in text])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jth_KTf6WCmQ",
        "colab_type": "code",
        "outputId": "8f1b5699-7a38-4196-fe40-558073ce3421",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "source": [
        "##Cada caracter tiene ahora asignado un entero, y esta relación se guardó en el diccionario 'char2idx'\n",
        "print('{')\n",
        "for char,_ in zip(char2idx, range(20)):\n",
        "  print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n",
        "print('  ...\\n')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\n",
            "  '\\n':   0,\n",
            "  ' ' :   1,\n",
            "  '!' :   2,\n",
            "  '$' :   3,\n",
            "  '&' :   4,\n",
            "  \"'\" :   5,\n",
            "  ',' :   6,\n",
            "  '-' :   7,\n",
            "  '.' :   8,\n",
            "  '3' :   9,\n",
            "  ':' :  10,\n",
            "  ';' :  11,\n",
            "  '?' :  12,\n",
            "  'A' :  13,\n",
            "  'B' :  14,\n",
            "  'C' :  15,\n",
            "  'D' :  16,\n",
            "  'E' :  17,\n",
            "  'F' :  18,\n",
            "  'G' :  19,\n",
            "  ...\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCK5xTFmX262",
        "colab_type": "code",
        "outputId": "4381c715-6857-47a2-c4a9-29af2435b26a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "##Muestra cómo los primeros 13 caracteres del texto están representados con enteros\n",
        "print('{}----caracteres representados como enteros---->{}'.format(repr(text[:13]), text_as_int[:13]))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'First Citizen'----caracteres representados como enteros---->[18 47 56 57 58  1 15 47 58 47 64 43 52]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4ZK3pADZpg0",
        "colab_type": "code",
        "outputId": "27579ad1-9534-48ee-8ac0-f68781780528",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "#La máxima longitud de las sentencias que queremos por una simple entrada de caracteres\n",
        "seq_length = 250\n",
        "examples_per_epoch = len(text)//seq_length\n",
        "\n",
        "##Se crea el ejemplo de entrenamiento\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)##Se usa la funcion tf.data.Dataset.from_tensor_slices()\n",
        "                                                              ##para convertir el vector texto en una secuencia de indices de caracter\n",
        "  \n",
        "for i in char_dataset.take(5):\n",
        "  print(idx2char[i.numpy()])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4tBwKYSdmYk",
        "colab_type": "code",
        "outputId": "5eafb093-2e3f-43b2-a58a-36e058f20eb2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for item in sequences.take(5):\n",
        "  print(repr(''.join(idx2char[item.numpy()])))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you know Caius Marcius is chief enemy to the people.\\n\\n'\n",
            "\"All:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us kill him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be done: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor c\"\n",
            "'itizens, the patricians good.\\nWhat authority surfeits on would relieve us: if they\\nwould yield us but the superfluity, while it were\\nwholesome, we might guess they relieved us humanely;\\nbut they think we are too dear: the leanness that\\nafflicts us, th'\n",
            "'e object of our misery, is as an\\ninventory to particularise their abundance; our\\nsufferance is a gain to them Let us revenge this with\\nour pikes, ere we become rakes: for the gods know I\\nspeak this in hunger for bread, not in thirst for revenge.\\n\\nSeco'\n",
            "\"nd Citizen:\\nWould you proceed especially against Caius Marcius?\\n\\nAll:\\nAgainst him first: he's a very dog to the commonalty.\\n\\nSecond Citizen:\\nConsider you what services he has done for his country?\\n\\nFirst Citizen:\\nVery well; and could be content to giv\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mf-Geqmfeg9Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_input_target(chunk):\n",
        "  input_text = chunk[:-1]\n",
        "  target_text = chunk[1:]\n",
        "  return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jf3YX-bgfVvL",
        "colab_type": "code",
        "outputId": "1ea37e81-4f67-4ecc-962b-24f37d3e8d27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "#Imprime el primer ejemplo de entrada y el valor resultante\n",
        "for input_example, target_example in dataset.take(1):\n",
        "  print('Dato de entrada: ', repr(''.join(idx2char[input_example.numpy()])))\n",
        "  print('Dato resultante: ', repr(''.join(idx2char[target_example.numpy()])))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dato de entrada:  'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you know Caius Marcius is chief enemy to the people.\\n'\n",
            "Dato resultante:  'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you know Caius Marcius is chief enemy to the people.\\n\\n'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VpiUI_HBgxqk",
        "colab_type": "code",
        "outputId": "8e2a23ec-d280-4a73-a2af-8a691db551db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
        "  print('Step {:4d}'.format(i))\n",
        "  print('    input: {} ({:s})'.format(input_idx, repr(idx2char[input_idx])))\n",
        "  print('    expected output: {} ({:s})'.format(target_idx, repr(idx2char[target_idx])))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step    0\n",
            "    input: 18 ('F')\n",
            "    expected output: 47 ('i')\n",
            "Step    1\n",
            "    input: 47 ('i')\n",
            "    expected output: 56 ('r')\n",
            "Step    2\n",
            "    input: 56 ('r')\n",
            "    expected output: 57 ('s')\n",
            "Step    3\n",
            "    input: 57 ('s')\n",
            "    expected output: 58 ('t')\n",
            "Step    4\n",
            "    input: 58 ('t')\n",
            "    expected output: 1 (' ')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSyTUQeYimVc",
        "colab_type": "code",
        "outputId": "38b14e90-e06d-4c45-86ab-3dbcc7a4a02c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "##Se crean los procesos de entrenamientos\n",
        "#Tamaño del proceso\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "##Tamaño del buffer para el dataset\n",
        "##(tf.data es designado para trabajar con infinitas posibilidades de secuencias)\n",
        "\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "dataset"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ((64, 250), (64, 250)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1xrSuUHkYQR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##PARA CONSTRUIR EL MODELO\n",
        "#Se usa tf.keras.Sequential para definir el modelo. Para este ejemplo simple\n",
        "#se usarán tres capas:\n",
        "#tf.keras.layers.Embedding, como capa de entrada. Una tabla entrenable que asigna cada entero de cada caracter a un vector \n",
        "#con cierta numero de dimensiones (256 en este caso, por el numero de caracteres ascii que existen)\n",
        "#tf.keras.layers.GRU: un tipo de RNN con tamaño units=rnn_units(Se puede usar una capa LSTM aquí)\n",
        "#tf.keras.layers.Dense: es la capa de salida, con vocab_size como salidas\n",
        "\n",
        "\n",
        "#Longitud del vocabulario en caracteres\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "#Dimensión de embedding\n",
        "embedding_dim = 256\n",
        "\n",
        "#Numero de unidades RNN\n",
        "rnn_units = 1024"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfChyDaumwIe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "      tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                               batch_input_shape=[batch_size, None]),\n",
        "      tf.keras.layers.LSTM(rnn_units, return_sequences=True,\n",
        "                          stateful = True,\n",
        "                          recurrent_initializer='glorot_uniform'),\n",
        "      tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sa40fTd179FC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = build_model(\n",
        "  vocab_size = len(vocab),\n",
        "  embedding_dim = embedding_dim,\n",
        "  rnn_units = rnn_units,\n",
        "  batch_size = BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LoPUNZds8Zzf",
        "colab_type": "code",
        "outputId": "2ce141c5-de8d-4c99-de42-a0d699bb03c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "##Ahora se corre el modelo para ver que se comporta como se espera\n",
        "##Primero chequea la forma de la salida\n",
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "  example_batch_predictions = model(input_example_batch)\n",
        "  print(example_batch_predictions.shape, '#(batch_size, sequence_length, vocab_size)')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 250, 65) #(batch_size, sequence_length, vocab_size)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpcSfdQI9vqc",
        "colab_type": "code",
        "outputId": "9707aa30-ffe7-45c0-9c33-f7257c80273e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "#En este ejemplo señalado arriba la longitud de la secuencia de entrada es 100 pero al modelo\n",
        "#puede serle introducido entradas de cualquier longitud\n",
        "model.summary()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (64, None, 256)           16640     \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (64, None, 1024)          5246976   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (64, None, 65)            66625     \n",
            "=================================================================\n",
            "Total params: 5,330,241\n",
            "Trainable params: 5,330,241\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MmaMUYoY-UuA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-aQhcCS_T_-",
        "colab_type": "code",
        "outputId": "8ed9d129-fd64-4513-e2ec-2bc2e57b5ddc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "sampled_indices"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([29, 10, 44, 53,  1, 19, 64, 35,  5, 56, 11, 42, 62,  8, 42, 21, 29,\n",
              "       20,  4, 10, 41, 49, 42,  3, 54,  8,  4, 31,  0, 16, 51, 40, 23, 11,\n",
              "       58, 54, 23, 32, 11, 11, 20, 64, 45, 57, 58, 21, 56, 63, 10, 29, 20,\n",
              "       56, 23, 18, 34, 39, 20, 11, 46, 61, 40, 57,  6, 33,  8, 40, 13, 49,\n",
              "       30, 57, 60,  8, 13, 37, 38, 32, 22, 20, 22, 43, 36, 20, 21, 58, 64,\n",
              "       20, 52, 30, 13, 10, 36, 47,  7, 51,  4, 15, 48, 33,  6, 32, 57,  8,\n",
              "        2, 18, 40, 59, 64, 27, 22, 53, 51, 59, 45, 45, 64,  0, 33, 56, 64,\n",
              "       25,  1, 31, 32,  4, 53, 10, 25, 48, 27, 54, 33, 52, 62, 45, 49, 55,\n",
              "       58, 15, 44,  6,  3, 18, 14, 23, 45, 56, 46, 35, 51, 29, 28, 11, 34,\n",
              "       63, 30, 43, 32, 45,  3, 27,  7, 36, 46, 22, 40, 16, 56, 34,  8, 52,\n",
              "       45, 13, 12, 63, 51, 37, 27, 24,  6, 10, 37, 56,  2, 32, 62,  6, 47,\n",
              "       26,  9, 29, 11, 63, 18,  1, 16, 21,  2, 22,  9, 37, 29,  1, 24, 41,\n",
              "       39, 59, 31, 14, 48, 36, 31, 59,  6, 48, 38, 20, 21, 55, 50, 10, 42,\n",
              "       22, 19, 56, 17, 46, 29, 24, 60,  0, 29, 37, 37, 40, 61, 37, 54, 33,\n",
              "       56, 45, 38, 30, 47,  9, 58,  6, 19,  9, 30, 20])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iAlCs0fD_dHR",
        "colab_type": "code",
        "outputId": "e3c05791-5033-4232-c1e3-2666105e0fbe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "#Decodifica para ver el texto predicho por el modelo no entrenado\n",
        "print('Input: \\n', repr(''.join(idx2char[input_example_batch[0]])))\n",
        "print()\n",
        "print('Siguientes caracteres predichos: \\n', repr(''.join(idx2char[sampled_indices ])))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: \n",
            " \"le\\nMust have their voices; neither will they bate\\nOne jot of ceremony.\\n\\nMENENIUS:\\nPut them not to't:\\nPray you, go fit you to the custom and\\nTake to you, as your predecessors have,\\nYour honour with your form.\\n\\nCORIOLANUS:\\nIt is apart\\nThat I shall blus\"\n",
            "\n",
            "Siguientes caracteres predichos: \n",
            " \"Q:fo GzW'r;dx.dIQH&:ckd$p.&S\\nDmbK;tpKT;;HzgstIry:QHrKFVaH;hwbs,U.bAkRsv.AYZTJHJeXHItzHnRA:Xi-m&CjU,Ts.!FbuzOJomuggz\\nUrzM ST&o:MjOpUnxgkqtCf,$FBKgrhWmQP;VyReTg$O-XhJbDrV.ngA?ymYOL,:Yr!Tx,iN3Q;yF DI!J3YQ LcauSBjXSu,jZHIql:dJGrEhQLv\\nQYYbwYpUrgZRi3t,G3RH\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uEaWcTOxAfAz",
        "colab_type": "code",
        "outputId": "b75b9690-ac59-490c-b617-0a5ae0f864fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#ENTRENANDO EL MODELO\n",
        "\n",
        "#La función estandar de cálculo de pérdida tf.keras.losses.sparse_categorical_crossentropy se usa en este caso\n",
        "#es aplicada a través de la última dimensión de predicción\n",
        "\n",
        "#Porque nuestro modelo retorna logits, necesitamos colocar el marcador from_logits\n",
        "\n",
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits = True)\n",
        "\n",
        "example_batch_loss = loss(target_example_batch, example_batch_predictions)\n",
        "print('Forma de la predicción: ', example_batch_predictions.shape, ' # (batch_size, sequence_length, vocab_size)')\n",
        "print('scalar_loss:            ', example_batch_loss.numpy().mean())"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Forma de la predicción:  (64, 250, 65)  # (batch_size, sequence_length, vocab_size)\n",
            "scalar_loss:             4.1742063\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UG61fL43DRUq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Se configura el procedimiento de entrenamiento usando tf.keras.Model.compile.\n",
        "#Usaremos tf.keras.optimizers.Adam con argumentos y función de pérdida por defecto\n",
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zcYr3hVDEPTB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Se usa tf.keras.callbacks.ModelCheckpoint para asegurarse que el checkpoint estará guardado durante el entrenamiento\n",
        "#DIrectorio donde el checkpoints estará guardado\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "#Nombre del archivo checkpoint\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, 'chpt_{epoch}')\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "  filepath=checkpoint_prefix,\n",
        "  save_weights_only=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lurwEzZ1FylY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#EJECUTA EL ENTRENAMIENTO\n",
        "#AUMENTAREMOS EL NUMERO DE EPOCAS A 30\n",
        "EPOCHS = 30"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFmy7ycLGEHV",
        "colab_type": "code",
        "outputId": "675a97d5-fa7c-4258-abf8-502300d2c4b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "69/69 [==============================] - 11s 157ms/step - loss: 3.1276\n",
            "Epoch 2/30\n",
            "69/69 [==============================] - 10s 144ms/step - loss: 2.3246\n",
            "Epoch 3/30\n",
            "69/69 [==============================] - 10s 144ms/step - loss: 2.0675\n",
            "Epoch 4/30\n",
            "69/69 [==============================] - 10s 145ms/step - loss: 1.8823\n",
            "Epoch 5/30\n",
            "69/69 [==============================] - 10s 143ms/step - loss: 1.7469\n",
            "Epoch 6/30\n",
            "69/69 [==============================] - 10s 143ms/step - loss: 1.6416\n",
            "Epoch 7/30\n",
            "69/69 [==============================] - 10s 144ms/step - loss: 1.5585\n",
            "Epoch 8/30\n",
            "69/69 [==============================] - 10s 143ms/step - loss: 1.4916\n",
            "Epoch 9/30\n",
            "69/69 [==============================] - 10s 145ms/step - loss: 1.4395\n",
            "Epoch 10/30\n",
            "69/69 [==============================] - 10s 145ms/step - loss: 1.3962\n",
            "Epoch 11/30\n",
            "69/69 [==============================] - 10s 143ms/step - loss: 1.3577\n",
            "Epoch 12/30\n",
            "69/69 [==============================] - 10s 143ms/step - loss: 1.3251\n",
            "Epoch 13/30\n",
            "69/69 [==============================] - 10s 142ms/step - loss: 1.2938\n",
            "Epoch 14/30\n",
            "69/69 [==============================] - 10s 143ms/step - loss: 1.2664\n",
            "Epoch 15/30\n",
            "69/69 [==============================] - 10s 143ms/step - loss: 1.2412\n",
            "Epoch 16/30\n",
            "69/69 [==============================] - 10s 143ms/step - loss: 1.2159\n",
            "Epoch 17/30\n",
            "69/69 [==============================] - 10s 143ms/step - loss: 1.1910\n",
            "Epoch 18/30\n",
            "69/69 [==============================] - 10s 143ms/step - loss: 1.1674\n",
            "Epoch 19/30\n",
            "69/69 [==============================] - 10s 142ms/step - loss: 1.1430\n",
            "Epoch 20/30\n",
            "69/69 [==============================] - 10s 142ms/step - loss: 1.1197\n",
            "Epoch 21/30\n",
            "69/69 [==============================] - 10s 143ms/step - loss: 1.0987\n",
            "Epoch 22/30\n",
            "69/69 [==============================] - 10s 142ms/step - loss: 1.0752\n",
            "Epoch 23/30\n",
            "69/69 [==============================] - 10s 142ms/step - loss: 1.0524\n",
            "Epoch 24/30\n",
            "69/69 [==============================] - 10s 143ms/step - loss: 1.0255\n",
            "Epoch 25/30\n",
            "69/69 [==============================] - 10s 142ms/step - loss: 0.9977\n",
            "Epoch 26/30\n",
            "69/69 [==============================] - 10s 142ms/step - loss: 0.9730\n",
            "Epoch 27/30\n",
            "69/69 [==============================] - 10s 143ms/step - loss: 0.9492\n",
            "Epoch 28/30\n",
            "69/69 [==============================] - 10s 142ms/step - loss: 0.9294\n",
            "Epoch 29/30\n",
            "69/69 [==============================] - 10s 142ms/step - loss: 0.9100\n",
            "Epoch 30/30\n",
            "69/69 [==============================] - 10s 142ms/step - loss: 0.8791\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VA_-Ud1sGaRn",
        "colab_type": "code",
        "outputId": "1c8a3384-4a2c-4f3d-c132-6c2797e4e757",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "##GENERACION DE TEXTO\n",
        "#para mantener este simple paso de predicción se usa un batch_size = 1\n",
        "#Para correr el modelo con diferente tamaño de lote(batch_size), necesitamos reconstruir\n",
        "#el modelo y restaurar el peso del checkpoint\n",
        "tf.train.latest_checkpoint(checkpoint_dir)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'./training_checkpoints/chpt_30'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bs61pFhJiVj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ouLUXLVMKDJN",
        "colab_type": "code",
        "outputId": "27a5efc8-b883-4024-dd7c-62842e5a8815",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (1, None, 256)            16640     \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (1, None, 1024)           5246976   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (1, None, 65)             66625     \n",
            "=================================================================\n",
            "Total params: 5,330,241\n",
            "Trainable params: 5,330,241\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-eVTjV1KGdN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#EL CICLO DE PREDICCION\n",
        "\n",
        "#El siguiente bloque de coóigo genera el texto\n",
        "#Comienza por escoger una cadena de inicio, inicializando la RNN y enviando el numero de caracteres a generar\n",
        "#Toma la distribución de predicción para el próximo caracter usando la cadena de inicio y el estado de la RNN\n",
        "#Usa una distribución categorial para calcular el índice del caracter predicho. Toma este caracter\n",
        "#como nueva entrada al modelo\n",
        "#EL estado de la RNN regresado por el modelo realimenta al modelo creando un contexto, en vez de una sola palabra\n",
        "#Este procedimiento se repite creando a medida que se realimenta, un aprendizaje y ampliando el contexto inicial\n",
        "#que permite una mejor predicción\n",
        "\n",
        "def generate_text(model, start_string):\n",
        "  #Evaluación (generación de texto usando el modelo de aprendizaje)\n",
        "  #Número de caracteres a generar\n",
        "  num_generate = 1000\n",
        "  \n",
        "  #Convirtiendo nuestra cadena de inicio a numeros(vectorizando)\n",
        "  input_eval = [char2idx[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "  \n",
        "  #Cadena vacía para guardar los resultados\n",
        "  text_generated = []\n",
        "  \n",
        "  #Baja temperatura resulta en texto más predecible\n",
        "  #Alta temperatura resulta en texto más sorprendente\n",
        "  #Experimento para hallar mejores resultados\n",
        "  temperature = 1.0\n",
        "  \n",
        "  #Aquí el batch_size == 1\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "    predictions = model(input_eval)\n",
        "    #Borra la dimension del proceso\n",
        "    predictions = tf.squeeze(predictions, 0)\n",
        "    \n",
        "    #Usando una distribución categórica para predecir la palabra regresada por el modelo\n",
        "    predictions = predictions / temperature\n",
        "    predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "    \n",
        "    #Pasamos la palabra predicha como la próxima entrada al modelo\n",
        "    input_eval = tf.expand_dims([predicted_id], 0)\n",
        "    \n",
        "    text_generated.append(idx2char[predicted_id])\n",
        "    \n",
        "  return (start_string + ''.join(text_generated))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZwVfSsgPJep",
        "colab_type": "code",
        "outputId": "cc4a5c68-36d0-4f79-976c-11891983c13f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        }
      },
      "source": [
        "print(generate_text(model, start_string=u'LEAR: '))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LEAR: MXEjTNHEPSj; FIIEENJW-ZIZ&FKbxENVAFMNISH$UkNXNXNCKPIT: KIMNISGOIODIULWFANDLIN&EHO!SF\n",
            "VANW:\n",
            "ces now, the rock is at the\n",
            "dukes; and as bett in sinsing armour rentine.\n",
            "\n",
            "SICINIUS:\n",
            "Hark! here impimen'd: ho, honest think, be guibtles which the\n",
            "house of Mine cut o'er the supposion: have you heard'st?\n",
            "I god! by Marsouse is mine honour's will.\n",
            "\n",
            "HORTENSIO:\n",
            "This is's tround-time is a friar or he,\n",
            "Out of the fortune's scholer of the base,\n",
            "But not inferio is rounwally;\n",
            "I am the sungear from the deather's love,\n",
            "And with divension, cousin depose,\n",
            "A priest he best within.\n",
            "\n",
            "LUCENTIO:\n",
            "Trit Lords of York looks of weeping souls,\n",
            "Fillwing of mine own virtues,\n",
            "They not rigning-man denied unkindly; and full\n",
            "proils inteer'd father; for no such fairest springs\n",
            "of these sorrow hath disperse these man of foul hours:\n",
            "Thou declatise have put\n",
            "zefold his witdor, and how fou'd itself?\n",
            "Then sees, to follow us: here, since away'd\n",
            "Uncomes a penshous and forget all.\n",
            "\n",
            "GLOUCESTER:\n",
            "Then fish surels, bid me bitience; mark yo\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRIbYusoPeu-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###\n",
        "###TAMBIÉN USANDO OTRA CADENA DE INICIO\n",
        "###O AÑADIENDO OTRA CAPA A LA RNN\n",
        "\n",
        "###USAREMOS EL tf.GradientTape\n",
        "\n",
        "#Primero se inicializa el estado de la RNN, llamando el método\n",
        "# tf.keras.Model.reset_states\n",
        "#Luego se itera sobre el dataset(lote por lote) y se calculan las predicciones asociadas con cada una\n",
        "#Se abre el tf.keras.GradientTape y se calculan las predicciones y pérdidas en ese contexto\n",
        "#Se calcula el gradiente de pérdida con respecto las variables modelo usando el método tf.Gradient.Tape.grads\n",
        "#Finalmente, se da un paso para usa el método tf.train.Optimizer.apply_gradients\n",
        "\n",
        "\n",
        "model = build_model(\n",
        "  vocab_size = len(vocab),\n",
        "  embedding_dim=embedding_dim,\n",
        "  rnn_units=rnn_units,\n",
        "  batch_size=BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bI3PFUNPS6mb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DSSnSdXpTFMC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp, target):\n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions = model(inp)\n",
        "    loss = tf.reduce_mean(\n",
        "      tf.keras.losses.sparse_categorical_crossentropy(\n",
        "        target, predictions, from_logits=True))\n",
        "  grads = tape.gradient(loss, model.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "  \n",
        "  return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHKPwCwnUCMf",
        "colab_type": "code",
        "outputId": "8259e7c5-bab6-4308-94e5-575bf5c225b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#Paso de entrenamiento\n",
        "EPOCHS = 10\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "  \n",
        "  ##Inicialización del estado oculto al inicio de cada epoch\n",
        "  ##La capa oculta inicialmente es None\n",
        "  hidden = model.reset_states()\n",
        "  \n",
        "  for (batch_n, (inp, target)) in enumerate(dataset):\n",
        "    loss = train_step(inp, target)\n",
        "    \n",
        "    if batch_n % 100 == 0:\n",
        "      template = 'Epoch {} Batch {} Loss {}'\n",
        "      print(template.format(epoch+1, batch_n, loss))\n",
        "      \n",
        "  #Guardando (checkpoint) el modelo cada 5 épocas\n",
        "  if (epoch + 1) % 5 == 0:\n",
        "    model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
        "    \n",
        "    \n",
        "  print('Epoca {} Pérdida {:.4f}'.format(epoch+1, loss))\n",
        "  print('Tiempo tomado por 1 época {} sec\\n'.format(time.time() - start))\n",
        "  \n",
        "model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
        "  "
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-0.embeddings\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.recurrent_kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-0.embeddings\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.recurrent_kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.bias\n",
            "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/alpha/guide/checkpoints#loading_mechanics for details.\n",
            "Epoch 1 Batch 0 Loss 4.174488544464111\n",
            "Epoca 1 Pérdida 2.5963\n",
            "Tiempo tomado por 1 época 11.757416248321533 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 2.569549798965454\n",
            "Epoca 2 Pérdida 2.2150\n",
            "Tiempo tomado por 1 época 9.44547414779663 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 2.2055587768554688\n",
            "Epoca 3 Pérdida 1.9882\n",
            "Tiempo tomado por 1 época 9.398260593414307 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 1.9997055530548096\n",
            "Epoca 4 Pérdida 1.8254\n",
            "Tiempo tomado por 1 época 9.339365482330322 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 1.8505524396896362\n",
            "Epoca 5 Pérdida 1.7047\n",
            "Tiempo tomado por 1 época 9.409043550491333 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 1.7243235111236572\n",
            "Epoca 6 Pérdida 1.6084\n",
            "Tiempo tomado por 1 época 9.36434030532837 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 1.626670002937317\n",
            "Epoca 7 Pérdida 1.5279\n",
            "Tiempo tomado por 1 época 9.359623193740845 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 1.551697850227356\n",
            "Epoca 8 Pérdida 1.4710\n",
            "Tiempo tomado por 1 época 9.358097791671753 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 1.4890209436416626\n",
            "Epoca 9 Pérdida 1.4143\n",
            "Tiempo tomado por 1 época 9.334773540496826 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 1.4440032243728638\n",
            "Epoca 10 Pérdida 1.3682\n",
            "Tiempo tomado por 1 época 9.450819969177246 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKPGxSvwXiEv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(generate_text(model, start_string=u'LEAR: '))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27XjNsrcZJDD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}